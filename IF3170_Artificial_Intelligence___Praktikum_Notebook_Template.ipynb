{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR1JW69eLfG_"
   },
   "source": [
    "# IF3170 Artificial Intelligence | Praktikum\n",
    "\n",
    "This notebook serves as a template for the assignment. Please create a copy of this notebook to complete your work. You can add more code blocks, markdown blocks, or new sections if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucbaI5rBLtjJ"
   },
   "source": [
    "Group Number: xx\n",
    "\n",
    "Group Members:\n",
    "- Name (NIM)\n",
    "- Name (NIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzsfETHLfHA"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jZJU5W_4LfHB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import other libraries if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbjLIdYLfHC"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-IWFJ-gdLfHD"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdSor5sdIYGs"
   },
   "source": [
    "# 1. Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a crucial step in the data analysis process that involves examining and visualizing data sets to uncover patterns, trends, anomalies, and insights. It is the first step before applying more advanced statistical and machine learning techniques. EDA helps you to gain a deep understanding of the data you are working with, allowing you to make informed decisions and formulate hypotheses for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bGiGPVYNIoWk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   N_Days         15000 non-null  float64\n",
      " 1   Drug           8450 non-null   object \n",
      " 2   Age            15000 non-null  float64\n",
      " 3   Sex            15000 non-null  object \n",
      " 4   Ascites        8453 non-null   object \n",
      " 5   Hepatomegaly   8448 non-null   object \n",
      " 6   Spiders        8441 non-null   object \n",
      " 7   Edema          15000 non-null  object \n",
      " 8   Bilirubin      15000 non-null  float64\n",
      " 9   Cholesterol    6626 non-null   float64\n",
      " 10  Albumin        15000 non-null  float64\n",
      " 11  Copper         8340 non-null   float64\n",
      " 12  Alk_Phos       8444 non-null   float64\n",
      " 13  SGOT           8441 non-null   float64\n",
      " 14  Tryglicerides  6575 non-null   float64\n",
      " 15  Platelets      14416 non-null  float64\n",
      " 16  Prothrombin    14984 non-null  float64\n",
      " 17  Stage          15000 non-null  float64\n",
      " 18  Status         15000 non-null  object \n",
      "dtypes: float64(12), object(7)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvx-gT3bLfHM"
   },
   "source": [
    "# 2. Split Training Set and Validation Set\n",
    "\n",
    "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
    "\n",
    "Note: For training, you should use the data contained in the `train.csv` given by the TA. The `test.csv` data is only used for kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "4yWCUFFBLfHM"
   },
   "outputs": [],
   "source": [
    "target_columns = ['Status']\n",
    "cat_columns = df_train.select_dtypes(include='object')\n",
    "indices_to_remove = [i for i, val in enumerate(cat_columns) if val in target_columns]\n",
    "num_columns = [x for x in df_train.columns if x not in cat_columns and x not in target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(target_columns, axis=1)\n",
    "y_train = df_train[target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC14lmo_LfHN"
   },
   "source": [
    "# 3. Data Cleaning and Preprocessing\n",
    "\n",
    "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
    "\n",
    "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
    "\n",
    "For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5rksSMAWICY_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.35"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[X_train.isna().any(axis=1)]) * 100 /len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column N_Days - 0.00% missing values\n",
      "Column Age - 0.00% missing values\n",
      "Column Bilirubin - 0.00% missing values\n",
      "Column Cholesterol - 56.03% missing values\n",
      "Column Albumin - 0.00% missing values\n",
      "Column Copper - 44.47% missing values\n",
      "Column Alk_Phos - 43.82% missing values\n",
      "Column SGOT - 43.84% missing values\n",
      "Column Tryglicerides - 56.42% missing values\n",
      "Column Platelets - 3.92% missing values\n",
      "Column Prothrombin - 0.09% missing values\n",
      "Column Stage - 0.00% missing values\n"
     ]
    }
   ],
   "source": [
    "missing_column = []\n",
    "for column in num_columns:\n",
    "    missing_percentage = X_train[column].isna().sum() / len(X_train) * 100\n",
    "    if missing_percentage > 0:\n",
    "        missing_column.append(column)\n",
    "    print(f\"Column {column} - {missing_percentage:.2f}% missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in missing_column:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ctVzt5DLfHd"
   },
   "source": [
    "# 3. Compile Preprocessing Pipeline\n",
    "\n",
    "All of the preprocessing classes or functions defined earlier will be compiled in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_ZlncSVjJG6"
   },
   "source": [
    "If you use sklearn to create preprocessing classes, you can list your preprocessing classes in the Pipeline object sequentially, and then fit and transform your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "jHraoW_7LfHd"
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Note: You can add or delete preprocessing components from this pipeline\n",
    "\n",
    "# pipe = Pipeline([(\"imputer\", FeatureImputer()),\n",
    "#                  (\"featurecreator\", FeatureCreator()),\n",
    "#                  (\"scaler\", FeatureScaler()),\n",
    "#                  (\"encoder\", FeatureEncoder())])\n",
    "\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9s56aFFxLfHd"
   },
   "outputs": [],
   "source": [
    "# # Your code should work up until this point\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXoCqMztjhr-"
   },
   "source": [
    "or create your own here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "7OoZ3oXEj2CW"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A3adbZXLfHe"
   },
   "source": [
    "# 4. Modeling and Validation\n",
    "\n",
    "Modelling is the process of building your own machine learning models to solve specific problems, or in this assignment context, predicting the probability for each class in the `Status` feature (`Status_C`, `Status_CL`, `Status_D`). Validation is the process of evaluating your trained model using the validation set or cross-validation method and providing some metrics that can help you decide what to do in the next iteration of development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnhMNbBILfHf"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "KV6ICmFmlqjk"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW0bMzkDLfHf"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "C_XwsN_-LfHg"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLDtIkPdLfHg"
   },
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "gZ6_x1LKLfHh"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP4TQDeRUUE0"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "bIjExgsPUYmM"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pg8aMBvoUWM-"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0E70wbTkUV58"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoH2u6fOLfHh"
   },
   "source": [
    "## Notes for improvements\n",
    "\n",
    "- **Visualize the model evaluation result**\n",
    "\n",
    "This will help you to understand the details more clearly about your model's performance. From the visualization, you can see clearly if your model is leaning towards a class than the others. (Hint: confusion matrix, ROC-AUC curve, etc.)\n",
    "\n",
    "- **Explore the hyperparameters of your models**\n",
    "\n",
    "Each models have their own hyperparameters. And each of the hyperparameter have different effects on the model behaviour. You can optimize the model performance by finding the good set of hyperparameters through a process called **hyperparameter tuning**. (Hint: Grid search, random search, bayesian optimization)\n",
    "\n",
    "- **Cross-validation**\n",
    "\n",
    "Cross-validation is a critical technique in machine learning and data science for evaluating and validating the performance of predictive models. It provides a more **robust** and **reliable** evaluation method compared to a hold-out (single train-test set) validation. Though, it requires more time and computing power because of how cross-validation works. (Hint: k-fold cross-validation, stratified k-fold cross-validation, etc.)\n",
    "\n",
    "- **Ensemble methods**\n",
    "\n",
    "Ensemble methods are powerful machine learning techniques that combine the predictions of multiple models (often referred to as base learners or weak learners) to create a stronger, more accurate predictive model. The idea behind ensemble methods is that by aggregating the opinions of multiple models, you can reduce the impact of individual model errors and improve overall prediction performance. (Hint: bagging, boosting, stacking, voting)\n",
    "\n",
    "- **Model interpretation**\n",
    "\n",
    "Model interpretation is the process of understanding and explaining the inner workings of a machine learning model, particularly its decision-making process. Interpretation helps data scientists, stakeholders, and end-users gain insights into why a model makes certain predictions or classifications. Model interpretation is crucial for building trust in machine learning systems, identifying biases, and extracting actionable information from models. (Hint: Feature importance, PDP, SHAP Values, etc)\n",
    "\n",
    "- **Explore other models**\n",
    "\n",
    "There are a lot of ML models that you can use in this usecase. Try to explore and use them to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li4l53DjLfHh"
   },
   "source": [
    "## Submission\n",
    "To predict the test set target feature and submit the results to the kaggle competition platform, do the following:\n",
    "1. Create a new pipeline instance identical to the first in Data Preprocessing\n",
    "2. With the pipeline, apply `fit_transform` to the original training set before splitting, then only apply `transform` to the test set.\n",
    "3. Retrain the model on the preprocessed training set\n",
    "4. Predict the test set\n",
    "5. Make sure the submission contains the `id`, `Status_C`, `Status_CL`, `Status_D` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "LeqnfWc-LfHi"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-jXvKOpLfHi"
   },
   "source": [
    "# 6. Error Analysis\n",
    "\n",
    "Based on all the process you have done until the modeling and evaluation step, write an analysis to support each steps you have taken to solve this problem. Write the analysis using the markdown block. Some questions that may help you in writing the analysis:\n",
    "\n",
    "- Does my model perform better in predicting one class than the other? If so, why is that?\n",
    "- To each models I have tried, which performs the best and what could be the reason?\n",
    "- Is it better for me to impute or drop the missing data? Why?\n",
    "- Does feature scaling help improve my model performance?\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWL3nEAELfHj"
   },
   "source": [
    "`Provide your analysis here`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
