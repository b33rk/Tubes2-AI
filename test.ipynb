{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF3170 Artificial Intelligence | Praktikum\n",
    "\n",
    "This notebook serves as a template for the assignment. Please create a copy of this notebook to complete your work. You can add more code blocks, markdown blocks, or new sections if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Number: xx\n",
    "\n",
    "Group Members:\n",
    "- Name (NIM)\n",
    "- Name (NIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import other libraries if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a crucial step in the data analysis process that involves examining and visualizing data sets to uncover patterns, trends, anomalies, and insights. It is the first step before applying more advanced statistical and machine learning techniques. EDA helps you to gain a deep understanding of the data you are working with, allowing you to make informed decisions and formulate hypotheses for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   N_Days         15000 non-null  float64\n",
      " 1   Drug           8450 non-null   object \n",
      " 2   Age            15000 non-null  float64\n",
      " 3   Sex            15000 non-null  object \n",
      " 4   Ascites        8453 non-null   object \n",
      " 5   Hepatomegaly   8448 non-null   object \n",
      " 6   Spiders        8441 non-null   object \n",
      " 7   Edema          15000 non-null  object \n",
      " 8   Bilirubin      15000 non-null  float64\n",
      " 9   Cholesterol    6626 non-null   float64\n",
      " 10  Albumin        15000 non-null  float64\n",
      " 11  Copper         8340 non-null   float64\n",
      " 12  Alk_Phos       8444 non-null   float64\n",
      " 13  SGOT           8441 non-null   float64\n",
      " 14  Tryglicerides  6575 non-null   float64\n",
      " 15  Platelets      14416 non-null  float64\n",
      " 16  Prothrombin    14984 non-null  float64\n",
      " 17  Stage          15000 non-null  float64\n",
      " 18  Status         15000 non-null  object \n",
      "dtypes: float64(12), object(7)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split Training Set and Validation Set\n",
    "\n",
    "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
    "\n",
    "Note: For training, you should use the data contained in the `train.csv` given by the TA. The `test.csv` data is only used for kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['Status']\n",
    "cat_columns = list(df_train.select_dtypes(include='object').columns) + list(['Stage'])\n",
    "indices_to_remove = [i for i, val in enumerate(cat_columns) if val in target_columns]\n",
    "cat_columns = np.delete(cat_columns, indices_to_remove)\n",
    "num_columns = [x for x in df_train.columns if x not in cat_columns and x not in target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(target_columns, axis=1)\n",
    "y_train = df_train[target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning and Preprocessing\n",
    "\n",
    "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
    "\n",
    "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
    "\n",
    "For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(X_train[X_train.isna().any(axis=1)]) * 100 /len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column N_Days - 0.00% missing values\n",
      "Column Age - 0.00% missing values\n",
      "Column Bilirubin - 0.00% missing values\n",
      "Column Cholesterol - 56.03% missing values\n",
      "Column Albumin - 0.00% missing values\n",
      "Column Copper - 44.47% missing values\n",
      "Column Alk_Phos - 43.82% missing values\n",
      "Column SGOT - 43.84% missing values\n",
      "Column Tryglicerides - 56.42% missing values\n",
      "Column Platelets - 3.92% missing values\n",
      "Column Prothrombin - 0.09% missing values\n"
     ]
    }
   ],
   "source": [
    "missing_column = []\n",
    "for column in num_columns:\n",
    "    missing_percentage = X_train[column].isna().sum() / len(X_train) * 100\n",
    "    if missing_percentage > 5:\n",
    "        missing_column.append(column)\n",
    "    print(f\"Column {column} - {missing_percentage:.2f}% missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Drug - 43.81% missing values\n",
      "Column Sex - 0.00% missing values\n",
      "Column Ascites - 43.77% missing values\n",
      "Column Hepatomegaly - 43.82% missing values\n",
      "Column Spiders - 43.84% missing values\n",
      "Column Edema - 0.00% missing values\n",
      "Column Stage - 0.00% missing values\n"
     ]
    }
   ],
   "source": [
    "for column in cat_columns:\n",
    "    missing_percentage = X_train[column].isna().sum() / len(X_train) * 100\n",
    "    if missing_percentage > 5:\n",
    "        missing_column.append(column)\n",
    "    print(f\"Column {column} - {missing_percentage:.2f}% missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platelets : 2.340967199225039\n"
     ]
    }
   ],
   "source": [
    "print('Platelets', \":\", df_train['Platelets'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(missing_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, isNum = False):\n",
    "        self.isNum = isNum\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        if (self.isNum):\n",
    "            simple_imputer = SimpleImputer(strategy='median')\n",
    "        else:\n",
    "            simple_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "        for col in X.columns:\n",
    "            missing_percentage = X[col].isna().sum() / len(X)\n",
    "            if missing_percentage < 0.1:\n",
    "                X[[col]] = simple_imputer.fit_transform(X[[col]])\n",
    "            else:\n",
    "                X = X.drop(col, axis=1)\n",
    "\n",
    "        return X\n",
    "\n",
    "missing_value_handler = MissingValueHandler()\n",
    "mo_X_train = missing_value_handler.fit_transform(X_train)\n",
    "# mo_X_train_df = pd.DataFrame(mo_X_train, columns=[x for x in (list(cat_columns) + list(num_columns)) if x not in missing_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4026     28018.0\n",
       "6104     26580.0\n",
       "11129    25568.0\n",
       "1732     25772.0\n",
       "582      25568.0\n",
       "          ...   \n",
       "14737    25569.0\n",
       "3073     25569.0\n",
       "6235     25568.0\n",
       "8433     26580.0\n",
       "10583    28650.0\n",
       "Name: Age, Length: 372, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['Age'].loc[X_train['Age']/365 > 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, isToCat = False):\n",
    "        self.isToCat = isToCat\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        if (not self.isToCat):\n",
    "            X['Age'].clip(upper = 70 * 365)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Note: You can add or delete preprocessing components from this pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['N_Days', 'Drug', 'Age', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders',\n",
       "       'Edema', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos',\n",
       "       'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "transformed_df = missing_value_handler.fit_transform(X_train[cat_columns])\n",
    "X_train[transformed_df.columns] = transformed_df\n",
    "# Train a Random Forest Classifier\n",
    "X_train.columns\n",
    "# rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "# rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get feature importances\n",
    "# importances = rf.feature_importances_\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'Feature': feature_names,\n",
    "#     'Importance': importances\n",
    "# }).sort_values(by='Importance', ascending=False)\n",
    "# print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot feature importances\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.title('Feature Importances')\n",
    "# plt.xlabel('Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compile Preprocessing Pipeline\n",
    "\n",
    "All of the preprocessing classes or functions defined earlier will be compiled in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use sklearn to create preprocessing classes, you can list your preprocessing classes in the Pipeline object sequentially, and then fit and transform your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Note: You can add or delete preprocessing components from this pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', MissingValueHandler(isNum=False)),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', MissingValueHandler(isNum=True)),\n",
    "    ('transformer', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "num_cat_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_columns),\n",
    "    (\"cat\", cat_pipeline, cat_columns)\n",
    "])\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('ageHandler', AgeTransformer()),\n",
    "    ('num_cat', num_cat_pipeline)\n",
    "])\n",
    "\n",
    "X_train_prepared = num_cat_pipeline.fit_transform(X_train, y_train)\n",
    "X_test_prepared = num_cat_pipeline.fit_transform(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Hepatomegaly</th>\n",
       "      <th>Spiders</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9839</th>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>Placebo</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7093</th>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13418</th>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Drug Sex Ascites Hepatomegaly Spiders Edema  Stage\n",
       "9839   D-penicillamine   F       N            Y       N     N    3.0\n",
       "9680           Placebo   F       N            N       N     N    2.0\n",
       "7093               NaN   F     NaN          NaN     NaN     N    3.0\n",
       "11293  D-penicillamine   F       N            N       N     N    3.0\n",
       "820    D-penicillamine   M       N            N       N     N    3.0\n",
       "...                ...  ..     ...          ...     ...   ...    ...\n",
       "5191               NaN   F     NaN          NaN     NaN     N    3.0\n",
       "13418              NaN   F     NaN          NaN     NaN     N    4.0\n",
       "5390               NaN   F     NaN          NaN     NaN     N    4.0\n",
       "860    D-penicillamine   F       N            N       N     N    3.0\n",
       "7270   D-penicillamine   F       N            N       N     N    3.0\n",
       "\n",
       "[12000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train[cat_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Your code should work up until this point\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or create your own here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling and Validation\n",
    "\n",
    "Modelling is the process of building your own machine learning models to solve specific problems, or in this assignment context, predicting the probability for each class in the `Status` feature (`Status_C`, `Status_CL`, `Status_D`). Validation is the process of evaluating your trained model using the validation set or cross-validation method and providing some metrics that can help you decide what to do in the next iteration of development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Initialize the LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# Fit and transform y_val to one-hot encoding\n",
    "y_val = lb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Type your code here\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = knn.predict_proba(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.66666667, 0.33333333, 0.        ],\n",
       "       [0.66666667, 0.        , 0.33333333],\n",
       "       ...,\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 3.2483402999805886\n"
     ]
    }
   ],
   "source": [
    "loss = log_loss(y_val, y_pred)\n",
    "print(f'Log Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Type your code here\n",
    "gaussiannb = GaussianNB()\n",
    "gaussiannb.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = knn.predict_proba(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.66666667, 0.33333333, 0.        ],\n",
       "       [0.66666667, 0.        , 0.33333333],\n",
       "       ...,\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 3.2483402999805886\n"
     ]
    }
   ],
   "source": [
    "loss = log_loss(y_val, y_pred)\n",
    "print(f'Log Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 8.470258546442532\n"
     ]
    }
   ],
   "source": [
    "# Type your code here\n",
    "knn = DecisionTreeClassifier()\n",
    "knn.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = knn.predict_proba(X_test_prepared)\n",
    "\n",
    "loss = log_loss(y_val, y_pred)\n",
    "print(f'Log Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.41823251905238945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Type your code here\n",
    "logisticregression = LogisticRegression()\n",
    "logisticregression.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = logisticregression.predict_proba(X_test_prepared)\n",
    "\n",
    "loss = log_loss(y_val, y_pred)\n",
    "print(f'Log Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the y_train labels to numerical labels\n",
    "y_train = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for improvements\n",
    "\n",
    "- **Visualize the model evaluation result**\n",
    "\n",
    "This will help you to understand the details more clearly about your model's performance. From the visualization, you can see clearly if your model is leaning towards a class than the others. (Hint: confusion matrix, ROC-AUC curve, etc.)\n",
    "\n",
    "- **Explore the hyperparameters of your models**\n",
    "\n",
    "Each models have their own hyperparameters. And each of the hyperparameter have different effects on the model behaviour. You can optimize the model performance by finding the good set of hyperparameters through a process called **hyperparameter tuning**. (Hint: Grid search, random search, bayesian optimization)\n",
    "\n",
    "- **Cross-validation**\n",
    "\n",
    "Cross-validation is a critical technique in machine learning and data science for evaluating and validating the performance of predictive models. It provides a more **robust** and **reliable** evaluation method compared to a hold-out (single train-test set) validation. Though, it requires more time and computing power because of how cross-validation works. (Hint: k-fold cross-validation, stratified k-fold cross-validation, etc.)\n",
    "\n",
    "- **Ensemble methods**\n",
    "\n",
    "Ensemble methods are powerful machine learning techniques that combine the predictions of multiple models (often referred to as base learners or weak learners) to create a stronger, more accurate predictive model. The idea behind ensemble methods is that by aggregating the opinions of multiple models, you can reduce the impact of individual model errors and improve overall prediction performance. (Hint: bagging, boosting, stacking, voting)\n",
    "\n",
    "- **Model interpretation**\n",
    "\n",
    "Model interpretation is the process of understanding and explaining the inner workings of a machine learning model, particularly its decision-making process. Interpretation helps data scientists, stakeholders, and end-users gain insights into why a model makes certain predictions or classifications. Model interpretation is crucial for building trust in machine learning systems, identifying biases, and extracting actionable information from models. (Hint: Feature importance, PDP, SHAP Values, etc)\n",
    "\n",
    "- **Explore other models**\n",
    "\n",
    "There are a lot of ML models that you can use in this usecase. Try to explore and use them to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "To predict the test set target feature and submit the results to the kaggle competition platform, do the following:\n",
    "1. Create a new pipeline instance identical to the first in Data Preprocessing\n",
    "2. With the pipeline, apply `fit_transform` to the original training set before splitting, then only apply `transform` to the test set.\n",
    "3. Retrain the model on the preprocessed training set\n",
    "4. Predict the test set\n",
    "5. Make sure the submission contains the `id`, `Status_C`, `Status_CL`, `Status_D` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Status_C</th>\n",
       "      <th>Status_CL</th>\n",
       "      <th>Status_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.140185</td>\n",
       "      <td>0.016349</td>\n",
       "      <td>0.843466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15001</td>\n",
       "      <td>0.775622</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.218533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15002</td>\n",
       "      <td>0.935002</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>0.061895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15003</td>\n",
       "      <td>0.901499</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.078854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15004</td>\n",
       "      <td>0.881654</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.109391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>24995</td>\n",
       "      <td>0.908816</td>\n",
       "      <td>0.019327</td>\n",
       "      <td>0.071856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>24996</td>\n",
       "      <td>0.634035</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>0.350321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>24997</td>\n",
       "      <td>0.124723</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.867582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>24998</td>\n",
       "      <td>0.010880</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.987458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>24999</td>\n",
       "      <td>0.916283</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>0.075193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  Status_C  Status_CL  Status_D\n",
       "0     15000  0.140185   0.016349  0.843466\n",
       "1     15001  0.775622   0.005845  0.218533\n",
       "2     15002  0.935002   0.003103  0.061895\n",
       "3     15003  0.901499   0.019647  0.078854\n",
       "4     15004  0.881654   0.008954  0.109391\n",
       "...     ...       ...        ...       ...\n",
       "9995  24995  0.908816   0.019327  0.071856\n",
       "9996  24996  0.634035   0.015644  0.350321\n",
       "9997  24997  0.124723   0.007695  0.867582\n",
       "9998  24998  0.010880   0.001661  0.987458\n",
       "9999  24999  0.916283   0.008524  0.075193\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_prepared = num_cat_pipeline.fit_transform(df_test)\n",
    "\n",
    "logisticregression = LogisticRegression()\n",
    "logisticregression.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = logisticregression.predict_proba(X_test_prepared)\n",
    "df_submission = pd.DataFrame(y_pred, columns=['Status_C', 'Status_CL', 'Status_D'])\n",
    "df_submission['id'] = df_test['id']\n",
    "df_submission = df_submission[['id', 'Status_C', 'Status_CL', 'Status_D']]\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_46948\\220050157.py\", line 2, in <module>\n",
      "    df_submission.to_csv('submission.csv', index=False)\n",
      "  File \"c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 211, in wrapper\n",
      "    raise TypeError(msg)\n",
      "  File \"c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 3720, in to_csv\n",
      "    str or None\n",
      "  File \"c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 211, in wrapper\n",
      "    raise TypeError(msg)\n",
      "  File \"c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py\", line 1162, in to_csv\n",
      "  File \"c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\", line 24, in <module>\n",
      "    from pandas._typing import SequenceNotStr\n",
      "ImportError: cannot import name 'SequenceNotStr' from 'pandas._typing' (c:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_typing.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\abdul\\AppData\\Roaming\\Python\\Python310\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Error Analysis\n",
    "\n",
    "Based on all the process you have done until the modeling and evaluation step, write an analysis to support each steps you have taken to solve this problem. Write the analysis using the markdown block. Some questions that may help you in writing the analysis:\n",
    "\n",
    "- Does my model perform better in predicting one class than the other? If so, why is that?\n",
    "- To each models I have tried, which performs the best and what could be the reason?\n",
    "- Is it better for me to impute or drop the missing data? Why?\n",
    "- Does feature scaling help improve my model performance?\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Provide your analysis here`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
